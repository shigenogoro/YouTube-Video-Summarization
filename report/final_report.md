# Cross-Dataset Video Chapter Generation and Temporal Alignment — MeetingBank & VidChapter-7M

Authors: Sheng-Kai Wen, Mohammad Derakshi

Date: December 2025

## Abstract

This report summarizes a two-phase project that implements and evaluates a modular pipeline to generate concise, timestamp-aligned video chapters from ASR transcripts. Phase 1 (MeetingBank) explored chaptering and summarization on meeting transcripts; Phase 2 used a VidChapter-7M subset (972 YouTube videos) to compare duration-based Fixed-Window segmentation and semantic adaptive segmentation. Chapter titles were produced by a fine-tuned BART model and aligned to timelines using a BERT-based classifier. Evaluation metrics include ROUGE, BERTScore, and Temporal F1 (±15s, ±30s). Across the VidChapter-7M subset, Fixed-Window segmentation proved a strong, scalable baseline.

## Introduction

Long-form video content (lectures, tutorials, meetings) is ubiquitous, but efficiently navigating such material remains challenging. Timestamp-aligned chapter summaries—short textual titles linked to specific time ranges—help users discover topics and jump to relevant parts without watching entire videos. This project builds a modular pipeline that converts ASR transcripts into short, timestamped chapter titles and aligns them to the video timeline.

The work covers two datasets in two phases. Phase 1 (MeetingBank) focuses on meeting transcripts and initial experiments in segmentation and summarization. Phase 2 (VidChapter-7M subset, 972 videos) implements two segmentation strategies: Fixed-Window (constant K=6) and Semantic Segmentation (adaptive K predicted by a GradientBoostingRegressor, with splits selected using Sentence-BERT similarity). Chapter titles are generated by a BART (base) model fine-tuned on segment–title pairs; a BERT-based alignment classifier maps titles back to timeline segments. We evaluate summarization quality (ROUGE, BERTScore) and alignment quality (Temporal F1 with ±15s and ±30s tolerance).

## Problem statement

Objective: design and evaluate a practical, modular pipeline that produces concise, timestamp-aligned chapter titles from ASR transcripts to improve navigation of long-form video content.

Research questions:

- Can a simple duration-based Fixed-Window segmentation match or outperform adaptive semantic segmentation for chapter generation and alignment on MeetingBank and VidChapter-7M?
- How well does a fine-tuned BART model generate concise, human-like chapter titles from transcript segments?
- How effectively can a BERT-based classifier align generated titles to timeline segments (measured by Temporal F1 at ±15s and ±30s)?

Scope and constraints:

- Inputs limited to ASR transcripts (no visual or raw audio features).
- Outputs are short chapter titles with start/end timestamps.
- Evaluation via ROUGE, BERTScore, Temporal F1, and segment-count prediction metrics.
- Emphasis on scalable, implementable methods rather than training expensive long-sequence transformers.

## What we proposed vs. what we accomplished

Below is a concise checklist of proposed tasks and the actual outcome.

- Proposed: Build a modular pipeline to produce timestamp-aligned chapter titles from ASR. — Completed: Pipeline implemented in notebooks (`notebooks/phase2_VidChapter-7M/02_VidChapter_Pipeline.ipynb`) and segmentation code (`03_VidChapter_Segment_version11.ipynb`). Models saved under `models/`.
- Proposed: Compare segmentation strategies (fixed-duration vs. semantic). — Completed: Implemented and evaluated Fixed-Window, Semantic-Segmentation (Adaptive K), and Semantic-Segmentation (Threshold) pipelines. Results reported in this repo and in Phase 2 report.
- Proposed: Predict adaptive segment count (K) for semantic segmentation. — Partially completed: Implemented a `GradientBoostingRegressor` for K prediction. Performance (MAE ≈ 4.58, exact-match ≈ 10.3%) limited general usefulness.
- Proposed: Develop a semantic-threshold segmentation method. — Completed: Implemented a threshold-based segmentation strategy using cosine similarity of Sentence-BERT embeddings.
- Proposed: Evaluate long-sequence transformer models (LongT5/LED). — Not completed: Omitted due to compute/time constraints; focused on BART as a practical summarization baseline.
- Proposed: Fine-tune a summarization model to generate chapter titles. — Completed: Fine-tuned BART (base) and evaluated its ROUGE and BERTScore results; best model saved as `models/vidchapter_bart_best`.
- Proposed: Implement temporal alignment of titles to timeline. — Completed: Implemented a BERT-based alignment classifier, evaluated with Temporal F1; model saved as `models/vidchapter_alignment`.
- Proposed: Evaluate on MeetingBank and VidChapter-7M. — Completed: Phase 1 (MeetingBank) and Phase 2 (VidChapter-7M subset) experiments performed; notebooks are in `notebooks/`.
- Proposed: Release code and artifacts. — Partially completed: Notebooks and model artifacts included in the repository; final packaging/documentation for public release is pending.

If an item is marked “Not completed” or “Partially completed”, the primary reasons were compute/time limits and the high cost of pairwise semantic computations for the full dataset.

## Datasets and preprocessing

- MeetingBank (Phase 1): Meeting transcripts (internal/collected dataset). Experiments focused on meeting-style, multi-party dialogue summarization and chaptering.
- VidChapter-7M subset (Phase 2): A subset of VidChapter-7M containing 972 videos with both user-provided chapters and ASR transcripts. Split: ~90% train (≈875 videos), ~10% test (≈97 videos).

Preprocessing steps (applied to both datasets where appropriate): normalize punctuation and whitespace; merge short consecutive ASR segments when necessary; truncate or pad segments for model inputs; compute per-video features for K-prediction (duration, word count, number of ASR lines, words-per-second). Sentence-BERT embeddings were computed per ASR chunk for semantic segmentation.

## Baselines

- Fixed-Window (K=6): split each transcript into 6 equal-duration windows; generate one title per window with fine-tuned BART.
- Semantic-Segmentation (Adaptive K): predict K per video using `GradientBoostingRegressor` then place K−1 split boundaries by selecting low-similarity points from Sentence-BERT embeddings.
- Semantic-Segmentation (Threshold): split transcript whenever cosine similarity between consecutive Sentence-BERT embeddings falls below a fixed threshold (0.4).

Hyperparameters and training details are described in the notebooks; high-level choices: BART-base fine-tuned for title generation with standard seq2seq cross-entropy and validation-based early stopping; BERT-based classifier fine-tuned on positive (ground-truth title+segment) and negative (random pair) examples.

## Approach and implementation

Pipeline overview:

1. Preprocess ASR transcripts into chunks (fixed windows or semantic splits).
2. For each chunk, generate a short title using a fine-tuned BART model.
3. Align generated titles to timeline segments using a BERT-based alignment classifier that ranks candidate segments; evaluate Temporal F1 by comparing predicted timestamps to ground-truth chapter timestamps within ±15s and ±30s tolerances.

Key implementation notes:

- Models and training scripts are in `notebooks/` (experimental notebooks). Major artifacts: `models/vidchapter_bart_best` (BART summarizer) and `models/vidchapter_alignment` (BERT classifier).
- Sentence-BERT (pretrained) was used for semantic similarity when computing segmentation candidates.
- K-prediction features: video duration, ASR word count, number of ASR subtitle lines, words-per-second.

Compute: experiments were run on available university GPUs and local machines; extensive semantic-threshold tuning was deferred because of the large number of pairwise similarity computations over many videos.

## Results (high-level summary)

Summarization (VidChapter-7M subset):
4.80%, ROUGE-2 = 6.56%, ROUGE-L = 14.31%, BERTScore F1 = 85.38%
- Semantic-Seg (Adaptive K): ROUGE-1 = 12.81%, ROUGE-2 = 5.10%, ROUGE-L = 12.51%, BERTScore F1 = 85.16%
- Semantic-Seg (Threshold=0.4): ROUGE-1 = 3.57%, ROUGE-2 = 0.91%, ROUGE-L = 3.50%, BERTScore F1 = 83.16%

Alignment (Temporal F1):

- Fixed-Window (K=6): F1 (±15s) = 65.35%, F1 (±30s) = 65.52%
- Semantic-Seg (Adaptive K): F1 (±15s) = 62.74%, F1 (±30s) = 64.66%
- Semantic-Seg (Threshold=0.4): F1 (±15s) = 39.51%, F1 (±30s) = 41.78%

K-prediction (Adaptive K): GradientBoostingRegressor achieved MAE ≈ 4.58 chapters; exact-match accuracy ≈ 10.29%.

Summary: Across the VidChapter-7M subset, Fixed-Window segmentation outperformed both semantic strategies (Adaptive and Threshold) on summarization and alignment metrics. The Threshold-based method performed significantly worse, suggesting that simple duration-based splitting is a more robust
Summary: Across the VidChapter-7M subset, Fixed-Window segmentation slightly outperformed the semantic adaptive strategy on both summarization and alignment metrics, suggesting that simple duration-based splitting is a robust and scalable baseline on this dataset.

## Error analysis

Manual inspection of failure cases suggests common issues:

- Segment granularity mismatch: user-provided chapters vary in length and semantics; fixed windows sometimes split human chapters, while semantic splits occasionally aggregate multiple human chapters.
- No visual signals: using ASR only limits the ability to detect visually-driven chapter boundaries (e.g., slide changes, demo segments).
- Noisy ASR: transcription errors (ASR noise) degrade both summarization and alignment quality.

We recommend a focused manual annotation of ~100 failed examples to categorize errors more precisely (e.g., ASR noise, semantic shift, visual cue boundary), which was not completed due to time constraints.

## Contributions of group members

- Sheng-Kai Wen:

- Mohammad Derakshi:

## Conclusion and future work

This rove semantic-threshold segmentation by dynamically selecting thresholds per video or using more sophisticated boundary detection algorithms (e.g., C99, TextTiling), as the fixed threshold approach underperformedway is that a simple Fixed-Window baseline is surprisingly competitive on the VidChapter-7M subset. Future directions:

- Implement and evaluate semantic-threshold segmentation to allow variable-length segments without heavy pairwise costs (investigate approximation techniques or streaming similarity measures).
- Incorporate lightweight visual cues (key frames, slide-change detectors) to improve boundary detection.
- Explore long-context transformers (LongT5, LED) or retrieval-augmented summarization if compute permits.

## AI Disclosure

## References
