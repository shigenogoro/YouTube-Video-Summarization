{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a6b581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clone the repo branch kyle (idempotent if you remove older clone)\n",
    "!git clone --branch kyle --single-branch https://github.com/shigenogoro/YouTube-Video-Summarization.git\n",
    "%cd YouTube-Video-Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8b3743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure repository root is on sys.path by locating the `src/` marker directory\n",
    "import sys, os\n",
    "from pathlib import Path\n",
    "\n",
    "def find_repo_root(marker='src'):\n",
    "    p = Path('.').resolve()\n",
    "    for candidate in [p] + list(p.parents):\n",
    "        if (candidate / marker).exists():\n",
    "            return candidate\n",
    "    return p\n",
    "\n",
    "repo_root = find_repo_root('src')\n",
    "if str(repo_root) not in sys.path:\n",
    "    sys.path.insert(0, str(repo_root))\n",
    "print('Repo root added to sys.path ->', repo_root)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51aa8d1",
   "metadata": {},
   "source": [
    "# Project Setup (modular)\n",
    "This notebook demonstrates a modular setup that imports from the `src/` package in this repository.\n",
    "It mirrors common Colab setup steps (config load, seed, dataset load, tokenization, model & trainer construction) while delegating logic to `src/` modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3728fd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config and small helpers\n",
    "from src.utils.io import load_yaml\n",
    "\n",
    "cfg_path = repo_root / 'configs' / 'model_bart_base.yaml'\n",
    "cfg = load_yaml(str(cfg_path))\n",
    "print('Loaded config keys:', list(cfg.keys()))\n",
    "# Expose commonly used pieces\n",
    "MODEL_NAME = cfg['model']['model_name']\n",
    "DATA_CFG = cfg['data']\n",
    "PREPROCESS_CFG = cfg.get('preprocess', {})\n",
    "TRAINING_CFG = cfg.get('training_args', cfg.get('training_args', {}))\n",
    "print('Model:', MODEL_NAME)\n",
    "print('Data config sample:', DATA_CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0247dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility using existing util\n",
    "from src.utils.seed import set_seed\n",
    "seed = TRAINING_CFG.get('seed', 42)\n",
    "set_seed(seed)\n",
    "print('Seed set to', seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd224d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load dataset via src.data.loaders; fall back to a small sample if files are missing\n",
    "from src.data.loaders import load_json_dataset\n",
    "from notebooks.helpers import sample_dataset\n",
    "\n",
    "try:\n",
    "    ds = load_json_dataset(DATA_CFG)\n",
    "    print('Dataset loaded with splits:', list(ds.keys()))\n",
    "except Exception as e:\n",
    "    print('Could not load dataset from disk/HF (falling back to sample):', e)\n",
    "    ds = {'train': sample_dataset(), 'validation': sample_dataset()}\n",
    "    print('Sample dataset created with columns:', ds['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e160048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model and tokenizer via existing module (this will download weights the first time)\n",
    "from src.models.build_model import build_model_and_tokenizer\n",
    "print('Model name to load:', MODEL_NAME)\n",
    "model, tokenizer = build_model_and_tokenizer(MODEL_NAME, cfg.get('model', {}))\n",
    "print('Model and tokenizer ready — tokenizer vocab size =', tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e10a7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize datasets using helpers.make_tokenize_fn and HF dataset.map (batched)\n",
    "from notebooks.helpers import make_tokenize_fn\n",
    "input_col = DATA_CFG.get('text_column', 'dialogue')\n",
    "summary_col = DATA_CFG.get('summary_column', 'summary')\n",
    "max_input = PREPROCESS_CFG.get('max_input_length', 1024)\n",
    "max_target = PREPROCESS_CFG.get('max_target_length', 256)\n",
    "tokenize_fn = make_tokenize_fn(tokenizer, input_col=input_col, target_col=summary_col, max_input_length=max_input, max_target_length=max_target)\n",
    "\n",
    "# If the loaded ds is a dict of Dataset objects (fallback created a dict), handle both cases\n",
    "def maybe_map(split):\n",
    "    d = split\n",
    "    try:\n",
    "        # HF Dataset object supports .map\n",
    "        tokenized = d.map(tokenize_fn, batched=True, remove_columns=d.column_names)\n",
    "        return tokenized\n",
    "    except Exception:\n",
    "        # If it's a dict-like (our fallback), assume it's already small and convert via map on Dataset\n",
    "        return d\n",
    "\n",
    "train_tok = maybe_map(ds['train'])\n",
    "eval_tok = maybe_map(ds.get('validation', ds.get('valid', ds.get('test', ds['train']))))\n",
    "print('Tokenization finished — sample keys for train tokenized:', list(train_tok.features.keys()) if hasattr(train_tok, 'features') else 'n/a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740b544e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a trainer object (won't start training here)\n",
    "from src.training.trainer import build_trainer\n",
    "trainer = build_trainer(model, tokenizer, train_tok, eval_tok, cfg)\n",
    "print('Trainer built. Trainer args output_dir =', trainer.args.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e547f7ce",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "- Run `trainer.train()` to start training (may require GPU and large disk/network downloads).\n",
    "- Move heavy dataset downloads and preprocessing to a separate notebook (`01_train.ipynb`).\n",
    "- Adjust `configs/model_bart_base.yaml` to point `data.path` to your local dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
