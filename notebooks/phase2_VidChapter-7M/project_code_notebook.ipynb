{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28dc250b",
   "metadata": {},
   "source": [
    "# VidChapter-7M: Chapter Generation with TinyLlama\n",
    "\n",
    "This notebook implements the training and inference pipeline for video chapter generation using the `lucas-ventura/chapter-llama` dataset and `TinyLlama/TinyLlama-1.1B-Chat-v1.0` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "891ad703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\CS_Knowledge\\FreeCodeCamp\\semantic-book-recommender\\venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from huggingface_hub import hf_hub_download\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ad662a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_REPO = \"lucas-ventura/chapter-llama\"\n",
    "\n",
    "# Small model for local experimentation\n",
    "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "# IMPORTANT: must be <= model.config.max_position_embeddings (2048 for TinyLlama)\n",
    "MAX_INPUT_TOKENS = 1000   # total sequence length (prompt + targets)\n",
    "\n",
    "# Keep transcripts modest so chapters fit too\n",
    "MAX_TRANSCRIPT_CHARS = 1500\n",
    "\n",
    "SEED = 42\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3386d71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seconds_to_hhmmss(t: float) -> str:\n",
    "    \"\"\"Convert seconds to HH:MM:SS (floor).\"\"\"\n",
    "    t = max(0, int(t))\n",
    "    h = t // 3600\n",
    "    m = (t % 3600) // 60\n",
    "    s = t % 60\n",
    "    return f\"{h:02d}:{m:02d}:{s:02d}\"\n",
    "\n",
    "\n",
    "def safe_get(d: Dict, key: str, default=None):\n",
    "    v = d.get(key, default)\n",
    "    return v if v is not None else default\n",
    "\n",
    "\n",
    "def load_json_from_hf(path_in_repo: str) -> Dict[str, Any]:\n",
    "    \"\"\"Download a JSON file from the HF dataset repo and load it.\"\"\"\n",
    "    local_path = hf_hub_download(repo_id=DATASET_REPO, filename=path_in_repo, repo_type=\"dataset\")\n",
    "    with open(local_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34cc6bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VidChaptersAsrDataset(Dataset):\n",
    "    \"\"\"\n",
    "    ASR-only dataset for Chapter-Llama style training.\n",
    "\n",
    "    Assumptions (based on the released subset JSONs):\n",
    "    - chapters_*.json:\n",
    "        {\n",
    "            \"<video_id>\": {\n",
    "             \"duration\": float,\n",
    "             \"title\": str,\n",
    "             \"description\": str,\n",
    "             \"channel_id\": str,\n",
    "             \"view_count\": int,\n",
    "             \"chapters\": {\n",
    "                \"<start_sec>\": \"<chapter title>\"\n",
    "                ...\n",
    "                }\n",
    "            },\n",
    "          ...\n",
    "        }\n",
    "\n",
    "    - asrs_*.json: supports TWO shapes:\n",
    "\n",
    "        (1) Dict-of-lists (as you observed):\n",
    "            {\n",
    "                \"<video_id>\": {\n",
    "                \"text\":  [str, ...],\n",
    "                \"start\": [float, ...],\n",
    "                \"end\":   [float, ...]\n",
    "            },\n",
    "          ...\n",
    "        }\n",
    "\n",
    "        (2) List-of-dicts (original assumption):\n",
    "            {\n",
    "                \"<video_id>\": [\n",
    "                    {\"start\": float, \"end\": float, \"text\": str},\n",
    "                    ...\n",
    "                    ],\n",
    "                ...\n",
    "            }\n",
    "\n",
    "    Each __getitem__ returns a dict with tokenized\n",
    "    input_ids, attention_mask, labels for a *single video*.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer,\n",
    "        chapters_json: Dict[str, Any],\n",
    "        asrs_json: Dict[str, Any],\n",
    "        max_input_tokens: int = 2048,\n",
    "        max_transcript_chars: int = 3000,\n",
    "        max_videos: int | None = None,\n",
    "    ):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.chapters_json = chapters_json\n",
    "        self.asrs_json = asrs_json\n",
    "        self.max_input_tokens = max_input_tokens\n",
    "        self.max_transcript_chars = max_transcript_chars\n",
    "\n",
    "        all_vids = sorted(set(chapters_json.keys()) & set(asrs_json.keys()))\n",
    "        if max_videos is not None:\n",
    "            all_vids = all_vids[:max_videos]\n",
    "        self.video_ids = all_vids\n",
    "\n",
    "        # Pre-tokenize static instruction skeleton to avoid recomputing its ids\n",
    "        self.base_instruction = (\n",
    "            \"You are an expert system that segments long YouTube videos into \"\n",
    "            \"semantically meaningful chapters and names each chapter.\\n\\n\"\n",
    "            \"Given the video title, description, and a subset of the ASR transcript \"\n",
    "            \"with timestamps, output ALL chapters for the video as lines of the form:\\n\"\n",
    "            \"HH:MM:SS - Chapter title\\n\\n\"\n",
    "            \"Be faithful to the content and avoid hallucinating chapters that are \"\n",
    "            \"not supported by the transcript.\\n\\n\"\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_ids)\n",
    "\n",
    "    def _build_transcript_text(self, asr_segments) -> str:\n",
    "        \"\"\"\n",
    "            Build a textual representation of ASR:\n",
    "\n",
    "            Handles:\n",
    "            - dict-of-lists: {\"text\": [...], \"start\": [...], \"end\": [...]}\n",
    "            - list-of-dicts: [{\"text\": ..., \"start\": ..., \"end\": ...}, ...]\n",
    "\n",
    "            Returns a single string like:\n",
    "              [HH:MM:SS] utterance\n",
    "              [HH:MM:SS] next utterance\n",
    "              ...\n",
    "            truncated by self.max_transcript_chars.\n",
    "        \"\"\"\n",
    "\n",
    "        triplets = []\n",
    "\n",
    "        if isinstance(asr_segments, dict) and all(\n",
    "            k in asr_segments for k in (\"text\", \"start\", \"end\")\n",
    "        ):\n",
    "            texts = asr_segments[\"text\"]\n",
    "            starts = asr_segments[\"start\"]\n",
    "            ends = asr_segments[\"end\"]\n",
    "            for t, s, e in zip(texts, starts, ends):\n",
    "                if not t:\n",
    "                    continue\n",
    "                try:\n",
    "                    s_float = float(s)\n",
    "                except Exception:\n",
    "                    s_float = 0.0\n",
    "                triplets.append((s_float, e, t))\n",
    "        else:\n",
    "            # Assume list[dict]\n",
    "            for seg in asr_segments:\n",
    "                text = seg.get(\"text\", \"\")\n",
    "                if not text:\n",
    "                    continue\n",
    "                try:\n",
    "                    s_float = float(seg.get(\"start\", 0.0))\n",
    "                except Exception:\n",
    "                    s_float = 0.0\n",
    "                triplets.append((s_float, seg.get(\"end\", 0.0), text))\n",
    "\n",
    "        triplets.sort(key=lambda x: x[0])  # sort by start time\n",
    "\n",
    "        chunks = []\n",
    "        total_chars = 0\n",
    "\n",
    "        for start, end, text in triplets:\n",
    "            ts = seconds_to_hhmmss(start)\n",
    "            line = f\"[{ts}] {text.strip()}\"\n",
    "            new_len = total_chars + len(line) + 1\n",
    "            if new_len > self.max_transcript_chars:\n",
    "                break\n",
    "            chunks.append(line)\n",
    "            total_chars = new_len\n",
    "\n",
    "        return \"\\n\".join(chunks)\n",
    "\n",
    "    def _build_chapter_target(self, chapters_dict: Dict[str, str]) -> str:\n",
    "        \"\"\"\n",
    "        Turn chapters dict into canonical text:\n",
    "        HH:MM:SS - Title\n",
    "        one per line, sorted by time.\n",
    "        \"\"\"\n",
    "        items = []\n",
    "        for start_str, title in chapters_dict.items():\n",
    "            try:\n",
    "                t = float(start_str)\n",
    "            except Exception:\n",
    "                # Sometimes keys might already be numeric-ish strings; fallback\n",
    "                try:\n",
    "                    t = float(str(start_str).replace(\",\", \"\"))\n",
    "                except Exception:\n",
    "                    t = 0.0\n",
    "            items.append((t, title))\n",
    "        items.sort(key=lambda x: x[0])\n",
    "        lines = [f\"{seconds_to_hhmmss(t)} - {title}\" for t, title in items]\n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Critical invariants:\n",
    "        - Total sequence length <= self.max_input_tokens\n",
    "        - There is at least ONE label token != -100\n",
    "        \"\"\"\n",
    "\n",
    "        # Safety loop: try a few different videos if current one is unusable\n",
    "        for _attempt in range(5):\n",
    "            vid = self.video_ids[idx]\n",
    "            chap_entry = self.chapters_json[vid]\n",
    "            asr_segments = self.asrs_json[vid]\n",
    "\n",
    "            video_title = safe_get(chap_entry, \"title\", \"\")\n",
    "            video_desc = safe_get(chap_entry, \"description\", \"\")\n",
    "            chapters_dict = safe_get(chap_entry, \"chapters\", {})\n",
    "\n",
    "            # Build transcript and target chapter text\n",
    "            transcript_text = self._build_transcript_text(asr_segments)\n",
    "            chapters_text = self._build_chapter_target(chapters_dict)\n",
    "\n",
    "            # ---- Prompt & target strings ----\n",
    "            prompt = (\n",
    "                self.base_instruction\n",
    "                + f\"Video title: {video_title}\\n\"\n",
    "                + f\"Video description: {video_desc}\\n\\n\"\n",
    "                + \"### Transcript (partial, chronologically ordered):\\n\"\n",
    "                + transcript_text\n",
    "                + \"\\n\\n### Chapters\\n\"\n",
    "            )\n",
    "            target = chapters_text\n",
    "\n",
    "            # ---- Separate tokenization for prompt/target ----\n",
    "            max_total = self.max_input_tokens\n",
    "            max_prompt = int(max_total * 0.75)   # allow prompt to take up to 75%\n",
    "            if max_prompt < 1:\n",
    "                max_prompt = max_total // 2\n",
    "\n",
    "            # Prompt\n",
    "            prompt_enc = self.tokenizer(\n",
    "                prompt,\n",
    "                add_special_tokens=False,\n",
    "                truncation=True,\n",
    "                max_length=max_prompt,\n",
    "            )\n",
    "            prompt_ids = prompt_enc[\"input_ids\"]\n",
    "\n",
    "            # Remaining room for target\n",
    "            max_target = max_total - len(prompt_ids)\n",
    "            if max_target <= 0:\n",
    "                # Prompt alone too long: keep only last chunk of prompt\n",
    "                prompt_ids = prompt_ids[-(max_total // 2):]\n",
    "                max_target = max_total - len(prompt_ids)\n",
    "\n",
    "            # Target (chapter lines)\n",
    "            target_enc = self.tokenizer(\n",
    "                target,\n",
    "                add_special_tokens=False,\n",
    "                truncation=True,\n",
    "                max_length=max_target,\n",
    "            )\n",
    "            target_ids = target_enc[\"input_ids\"]\n",
    "\n",
    "            # If no target tokens → this sample is useless; try another video\n",
    "            if len(target_ids) == 0:\n",
    "                idx = (idx + 1) % len(self.video_ids)\n",
    "                continue\n",
    "\n",
    "            input_ids = prompt_ids + target_ids\n",
    "            attention_mask = [1] * len(input_ids)\n",
    "            labels = [-100] * len(prompt_ids) + target_ids\n",
    "\n",
    "            # Safety: truncate everything to max_total\n",
    "            if len(input_ids) > max_total:\n",
    "                input_ids = input_ids[:max_total]\n",
    "                attention_mask = attention_mask[:max_total]\n",
    "                labels = labels[:max_total]\n",
    "\n",
    "            # Final check: ensure at least one token has a real label\n",
    "            if all(l == -100 for l in labels):\n",
    "                idx = (idx + 1) % len(self.video_ids)\n",
    "                continue\n",
    "\n",
    "            return {\n",
    "                \"input_ids\": input_ids,\n",
    "                \"attention_mask\": attention_mask,\n",
    "                \"labels\": labels,\n",
    "                \"video_id\": vid,\n",
    "            }\n",
    "\n",
    "        # If we somehow failed 5 times in a row:\n",
    "        raise RuntimeError(\"Failed to construct a valid training example with at least one label token.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed41167e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ChapteringCollator:\n",
    "    tokenizer: Any\n",
    "\n",
    "    def __call__(self, batch: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
    "        # Variable-length → pad\n",
    "        max_len = max(len(ex[\"input_ids\"]) for ex in batch)\n",
    "        pad_id = self.tokenizer.pad_token_id\n",
    "\n",
    "        input_ids = []\n",
    "        attention_mask = []\n",
    "        labels = []\n",
    "\n",
    "        for ex in batch:\n",
    "            n = len(ex[\"input_ids\"])\n",
    "            pad_len = max_len - n\n",
    "\n",
    "            input_ids.append(ex[\"input_ids\"] + [pad_id] * pad_len)\n",
    "            attention_mask.append(ex[\"attention_mask\"] + [0] * pad_len)\n",
    "            labels.append(ex[\"labels\"] + [-100] * pad_len)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.long),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670a1679",
   "metadata": {},
   "source": [
    "## Display Example Data\n",
    "Here we load the training data and display one example to understand the structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73681193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading subset JSONs from Hugging Face...\n",
      "Example Video ID: -1vmZJ-EWtI\n",
      "Chapter Data:\n",
      "{\n",
      "  \"duration\": 2595.906,\n",
      "  \"title\": \"If You Ever Experience Anxiety, Try These Tips to Overcome It | Seane Corn on Women of Impact\",\n",
      "  \"description\": \"Hey guys, Lisa here! If you didn\\u2019t already know, I am super frikin excited to share that I\\u2019m writing a book! To be the FIRST to get sneak peeks about my book and other exclusive content go to: http://lisabilyeu.com/ and be sure to sign up for my newsletter.\\n\\nSeane Corn overcame OCD and severe anxiety, not to mention drug abuse, through yoga and spiritual practice. Now she is part of the revolution of the soul, practicing radical healing, conscious action, and guiding people to self-awareness. On this episode of Women of Impact with Lisa Bilyeu, Seane Corn shares her history with trauma, explains how to identify and change the stories we tell ourselves, and describes how she healed herself with yoga.\\n\\nSHOW NOTES:\\n\\nSeane shares her history with OCD, anxiety and drug abuse [3:00]\\nSeane explains what caused her to move away from drugs and towards yoga [8:21]\\nSeane describes what kept her going when yoga at first made anxiety worse [12:26]\\nSeane and Lisa discuss trauma and the misconceptions surrounding it [14:23]\\nSeane talks about the connection between mind, body and soul [20:58]\\nSeane explains how to identify and unravel the stories we tell ourselves [23:43]\\nSeane rejects \\u201cwhite saviorism\\u201d and confronts her own power and privilege [27:00]\\nSeane advocates confronting your own participation in oppressive systems [31:18]\\nSeane describes the process for dealing with harmful attitudes and behaviors [33:30]\\nSeane explains how to understand intergenerational trauma [37:09]\\nSeane hopes that her book inspires people to love and serve peace [38:32]\\nSeane shares her superpower [40:15]\\n\\nFOLLOW SEANE:\\n\\nWEBSITE: https://www.seanecorn.com\\nINSTAGRAM: https://bit.ly/2lFemdH\\nFACEBOOK: https://bit.ly/2lMOHja\\nTWITTER: https://bit.ly/2kFaqt9\",\n",
      "  \"channel_id\": \"UCeir7Wbzzfg43c1eL7PSa3g\",\n",
      "  \"view_count\": 34853,\n",
      "  \"chapters\": {\n",
      "    \"180\": \"Seane shares her history with OCD, anxiety and drug abuse\",\n",
      "    \"501\": \"Seane explains what caused her to move away from drugs and towards yoga\",\n",
      "    \"746\": \"Seane describes what kept her going when yoga at first made anxiety worse\",\n",
      "    \"863\": \"Seane and Lisa discuss trauma and the misconceptions surrounding it\",\n",
      "    \"1258\": \"Seane talks about the connection between mind, body and soul\",\n",
      "    \"1423\": \"Seane explains how to identify and unravel the stories we tell ourselves\",\n",
      "    \"1620\": \"Seane rejects \\u201cwhite saviorism\\u201d and confronts her own power and privilege\",\n",
      "    \"1878\": \"Seane advocates confronting your own participation in oppressive systems\",\n",
      "    \"2010\": \"Seane describes the process for dealing with harmful attitudes and behaviors\",\n",
      "    \"2229\": \"Seane explains how to understand intergenerational trauma\",\n",
      "    \"2312\": \"Seane hopes that her book inspires people to love and serve peace\",\n",
      "    \"2415\": \"Seane shares her superpower\"\n",
      "  }\n",
      "}\n",
      "\n",
      "ASR Data (first 2 segments):\n",
      "{\n",
      "  \"start\": [\n",
      "    0.21,\n",
      "    2.76\n",
      "  ],\n",
      "  \"end\": [\n",
      "    2.76,\n",
      "    5.46\n",
      "  ],\n",
      "  \"text\": [\n",
      "    \"what does Sally Field Glenn and Doyle\",\n",
      "    \"Gabby Bernstein Marianne Williamson and\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading subset JSONs from Hugging Face...\")\n",
    "\n",
    "chapters_train = load_json_from_hf(\"docs/subset_data/chapters/chapters_sml1k_train.json\")\n",
    "asrs_train = load_json_from_hf(\"docs/subset_data/asrs/asrs_sml1k_train.json\")\n",
    "\n",
    "# Display one example\n",
    "example_vid_id = list(chapters_train.keys())[0]\n",
    "print(f\"Example Video ID: {example_vid_id}\")\n",
    "print(\"Chapter Data:\")\n",
    "print(json.dumps(chapters_train[example_vid_id], indent=2))\n",
    "print(\"\\nASR Data (first 2 segments):\")\n",
    "if isinstance(asrs_train[example_vid_id], dict):\n",
    "    print(json.dumps({k: v[:2] for k, v in asrs_train[example_vid_id].items()}, indent=2))\n",
    "else:\n",
    "    print(json.dumps(asrs_train[example_vid_id][:2], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790b7da3",
   "metadata": {},
   "source": [
    "## Model Setup and Training\n",
    "Initialize the model, tokenizer, and trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a391434c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Model max position embeddings: 2048\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-21): 22 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Device (MPS on Apple Silicon if available)\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load tokenizer & model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.model_max_length = MAX_INPUT_TOKENS\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.config.use_cache = False  # important for training\n",
    "\n",
    "max_ctx = model.config.max_position_embeddings\n",
    "print(\"Model max position embeddings:\", max_ctx)\n",
    "assert MAX_INPUT_TOKENS <= max_ctx\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d2d2112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#train videos: 50\n",
      "#val videos:   50\n"
     ]
    }
   ],
   "source": [
    "chapters_val = load_json_from_hf(\"docs/subset_data/chapters/chapters_sml300_val.json\")\n",
    "asrs_val = load_json_from_hf(\"docs/subset_data/asrs/asrs_sml300_val.json\")\n",
    "\n",
    "# Build datasets\n",
    "train_dataset = VidChaptersAsrDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    chapters_json=chapters_train,\n",
    "    asrs_json=asrs_train,\n",
    "    max_input_tokens=MAX_INPUT_TOKENS,\n",
    "    max_transcript_chars=MAX_TRANSCRIPT_CHARS,\n",
    "    max_videos=50,  # or e.g. 200 for very quick smoke test\n",
    ")\n",
    "\n",
    "val_dataset = VidChaptersAsrDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    chapters_json=chapters_val,\n",
    "    asrs_json=asrs_val,\n",
    "    max_input_tokens=MAX_INPUT_TOKENS,\n",
    "    max_transcript_chars=MAX_TRANSCRIPT_CHARS,\n",
    "    max_videos=50,\n",
    ")\n",
    "\n",
    "print(f\"#train videos: {len(train_dataset)}\")\n",
    "print(f\"#val videos:   {len(val_dataset)}\")\n",
    "\n",
    "collator = ChapteringCollator(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "078163d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gorow\\AppData\\Local\\Temp\\ipykernel_29340\\1902355715.py:35: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "d:\\CS_Knowledge\\FreeCodeCamp\\semantic-book-recommender\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 1:09:28, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Model saved to: outputs/chapter_llama_asr_sml1k_tiny\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# Training args\n",
    "# -------------------------\n",
    "output_dir = \"outputs/chapter_llama_asr_sml1k_tiny\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Conservative settings for stability on M4 + TinyLlama\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=1.0,\n",
    "    learning_rate=2e-5,\n",
    "    warmup_ratio=0.03,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\",\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    gradient_checkpointing=False,  # keep off for now for stability\n",
    "    remove_unused_columns=False,\n",
    "    max_grad_norm=1.0,             # gradient clipping\n",
    ")\n",
    "\n",
    "# Simple metric: validation loss (Trainer logs eval_loss) + we can add perplexity later\n",
    "def compute_metrics(eval_pred):\n",
    "    # logits, labels = eval_pred\n",
    "    return {}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"Done.\")\n",
    "\n",
    "# Save final model & tokenizer\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "print(f\"Model saved to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c678b627",
   "metadata": {},
   "source": [
    "## Inference\n",
    "Run inference on a held-out video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea759f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running quick inference on one held-out test video ===\n",
      "[INFER] Using device: cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b655570d260a4c7495d3095e62d19956",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "docs/subset_data/chapters/chapters_s_tes(…):   0%|          | 0.00/12.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CS_Knowledge\\FreeCodeCamp\\semantic-book-recommender\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\gorow\\.cache\\huggingface\\hub\\datasets--lucas-ventura--chapter-llama. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fa858e387bc4e12a819d5c3069bcdcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "docs/subset_data/asrs/asrs_s_test.json:   0%|          | 0.00/50.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "[INFER] VIDEO ID: --7febZVhIs\n",
      "================================================================================\n",
      "\n",
      "[INFER] TRANSCRIPT (first ~15 lines):\n",
      "[00:00:02] Hey, what's up everybody?\n",
      "[00:00:07] My name is Ray and I'm the creator of Ray So Silly, a YouTube channel dedicated to creating videos with green screen technology.\n",
      "[00:00:14] I'm excited to be here on KineMaster's channel to show you how to use the KineMaster app\n",
      "[00:00:19] to achieve the green screen effect.\n",
      "[00:00:21] This is a popular effect used in many television shows, movies, and more.\n",
      "[00:00:26] First, I'm going to show you how to achieve this effect, and then you can use your imagination to create a world of your own.\n",
      "[00:00:32] Let's open up the app and get started.\n",
      "[00:00:34] Once you're in KineMaster, tap the plus icon here to create new project.\n",
      "[00:00:39] In here, choose the aspect ratio.\n",
      "[00:00:41] I'm going to choose 16 by 9.\n",
      "[00:00:44] Now click on media to go into your camera roll to select the video.\n",
      "[00:00:48] Now from the video, switch to photo.\n",
      "[00:00:50] And here we need a clear green photo ready to track to help with this effect.\n",
      "[00:00:56] After importing the photo, click layers, then media, and click all.\n",
      "[00:01:01] Now import your video.\n",
      "... (transcript truncated) ...\n",
      "\n",
      "[INFER] GROUND-TRUTH CHAPTERS:\n",
      "00:00:00 - Introduction\n",
      "00:00:34 - Starting a New Project\n",
      "00:00:44 - Adding a Background\n",
      "00:00:58 - Adding a Layer\n",
      "00:01:08 - Using Split Screen\n",
      "00:01:18 - Using Chroma Key\n",
      "00:01:31 - Exporting a Video Project\n",
      "00:01:42 - Creating a Composite Project\n",
      "00:02:02 - Using Chroma Key\n",
      "00:02:15 - Exporting a Second Time\n",
      "00:02:35 - Find More on the KineMaster Asset Store\n",
      "00:02:45 - Conclusion\n",
      "\n",
      "[INFER] PREDICTED CHAPTERS:\n",
      "a Key.\n",
      "00:01:34 - Starting a New Project\n",
      "00:00:34 - Adding a Background\n",
      "00:00:44 - Adding a Layer\n",
      "00:00:58 - Adding a Layer\n",
      "00:01:08 - Using Split Screen\n",
      "00:01:18 - Using Chroma Key\n",
      "00:01:31 - Exporting a Video Project\n",
      "00:01:42 - Creating a Composite Project\n",
      "00:02:02 - Using Chroma Key\n",
      "00:02:15 - Exporting a Second Time\n",
      "00:02:35 - Find More on the KineMaster Asset Store\n",
      "00:02:45 - Conclusion\n",
      "00:00:00 - Introduction\n",
      "00:00:02 - The Second Chance\n",
      "00:00:07 - My name is Ray\n",
      "00:00:14 - Starting a New Project\n",
      "00:00:34 - Adding a Background\n",
      "00:00:44 -\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "def run_single_video_inference(\n",
    "    model_dir: str,\n",
    "    subset_tag: str = \"s\",\n",
    "    max_new_tokens: int = 256,\n",
    "):\n",
    "    \"\"\"\n",
    "    Load a fine-tuned checkpoint and run chaptering on ONE held-out video\n",
    "    from chapters_{subset_tag}_test.json + asrs_{subset_tag}_test.json.\n",
    "\n",
    "    subset_tag=\"s\" → use:\n",
    "      docs/subset_data/chapters/chapters_s_test.json\n",
    "      docs/subset_data/asrs/asrs_s_test.json\n",
    "    \"\"\"\n",
    "\n",
    "    # --------------------------\n",
    "    # Device & model\n",
    "    # --------------------------\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    print(f\"[INFER] Using device: {device}\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_dir)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # --------------------------\n",
    "    # Load test JSONs\n",
    "    # --------------------------\n",
    "    chapters_test = load_json_from_hf(\n",
    "        f\"docs/subset_data/chapters/chapters_{subset_tag}_test.json\"\n",
    "    )\n",
    "    asrs_test = load_json_from_hf(\n",
    "        f\"docs/subset_data/asrs/asrs_{subset_tag}_test.json\"\n",
    "    )\n",
    "\n",
    "    # Tiny dataset wrapper to reuse formatting helpers\n",
    "    test_dataset = VidChaptersAsrDataset(\n",
    "        tokenizer=tokenizer,\n",
    "        chapters_json=chapters_test,\n",
    "        asrs_json=asrs_test,\n",
    "        max_input_tokens=MAX_INPUT_TOKENS,\n",
    "        max_transcript_chars=MAX_TRANSCRIPT_CHARS,\n",
    "        max_videos=None,\n",
    "    )\n",
    "\n",
    "    if len(test_dataset) == 0:\n",
    "        raise ValueError(\"[INFER] No test videos found.\")\n",
    "\n",
    "    vid0 = test_dataset.video_ids[0]\n",
    "    chap_entry = chapters_test[vid0]\n",
    "    asr_segments = asrs_test[vid0]\n",
    "\n",
    "    video_title = chap_entry.get(\"title\", \"\")\n",
    "    video_desc = chap_entry.get(\"description\", \"\")\n",
    "    chapters_dict = chap_entry.get(\"chapters\", {})\n",
    "\n",
    "    # Reuse dataset helpers for text format\n",
    "    transcript_text = test_dataset._build_transcript_text(asr_segments)\n",
    "    gt_chapters_text = test_dataset._build_chapter_target(chapters_dict)\n",
    "    base_instruction = test_dataset.base_instruction\n",
    "\n",
    "    # Build prompt exactly like during training (minus target)\n",
    "    prompt = (\n",
    "        base_instruction\n",
    "        + f\"Video title: {video_title}\\n\"\n",
    "        + f\"Video description: {video_desc}\\n\\n\"\n",
    "        + \"### Transcript (partial, chronologically ordered):\\n\"\n",
    "        + transcript_text\n",
    "        + \"\\n\\n### Chapters\\n\"\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_INPUT_TOKENS,\n",
    "    )\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        gen_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            num_beams=1,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    # Cut off the prompt, keep only new tokens\n",
    "    input_len = inputs[\"input_ids\"].shape[1]\n",
    "    gen_only = gen_ids[0][input_len:]\n",
    "    pred_text = tokenizer.decode(gen_only, skip_special_tokens=True)\n",
    "\n",
    "    # Pretty-print\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"[INFER] VIDEO ID: {vid0}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    print(\"\\n[INFER] TRANSCRIPT (first ~15 lines):\")\n",
    "    t_lines = transcript_text.splitlines()\n",
    "    for line in t_lines[:15]:\n",
    "        print(line)\n",
    "    if len(t_lines) > 15:\n",
    "        print(\"... (transcript truncated) ...\")\n",
    "\n",
    "    print(\"\\n[INFER] GROUND-TRUTH CHAPTERS:\")\n",
    "    print(gt_chapters_text)\n",
    "\n",
    "    print(\"\\n[INFER] PREDICTED CHAPTERS:\")\n",
    "    print(pred_text)\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "# Run inference\n",
    "print(\"\\n=== Running quick inference on one held-out test video ===\")\n",
    "run_single_video_inference(\n",
    "    model_dir=output_dir,\n",
    "    subset_tag=\"s\",         # uses chapters_s_test / asrs_s_test\n",
    "    max_new_tokens=256,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9a1ac2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
