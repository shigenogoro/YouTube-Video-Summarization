{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2cc4c52",
   "metadata": {},
   "source": [
    "# Baseline Implementation: BART\n",
    "\n",
    "- Load data: `01_data_exploration`\n",
    "\n",
    "- Data Preprocessing\n",
    "\n",
    "- Baseline Model: BART"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c953568",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "Copy from `01_data_exploration.ipynb`\n",
    "\n",
    "- Github Setup\n",
    "\n",
    "- Packages Setup\n",
    "\n",
    "- Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc502c87",
   "metadata": {},
   "source": [
    "### Github Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5dd99fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'YouTube-Video-Summarization'...\n",
      "remote: Enumerating objects: 91, done.\u001b[K\n",
      "remote: Counting objects: 100% (91/91), done.\u001b[K\n",
      "remote: Compressing objects: 100% (59/59), done.\u001b[K\n",
      "remote: Total 91 (delta 28), reused 82 (delta 20), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (91/91), 275.54 KiB | 21.19 MiB/s, done.\n",
      "Resolving deltas: 100% (28/28), done.\n",
      "/content/YouTube-Video-Summarization/YouTube-Video-Summarization\n"
     ]
    }
   ],
   "source": [
    "username = 'shigenogoro'\n",
    "repo_name = 'YouTube-Video-Summarization'\n",
    "branch = 'kyle'\n",
    "\n",
    "# Check if the destination directory already exists\n",
    "import os\n",
    "if os.path.exists(repo_name):\n",
    "    print(f\"The directory '{repo_name}' already exists. Skipping clone.\")\n",
    "    \n",
    "    # Pull the latest changes\n",
    "    %cd {repo_name}\n",
    "    !git pull origin {branch}\n",
    "else:\n",
    "    # Clone the repository \n",
    "    !git clone https://github.com/{username}/{repo_name}.git -b {branch}\n",
    "    \n",
    "    # Change directory to the cloned repository\n",
    "    %cd {repo_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6b64b4",
   "metadata": {},
   "source": [
    "### Packages Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73ae9985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 1)) (2.0.2)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (2.2.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 3)) (4.67.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 4)) (3.10.0)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 5)) (0.13.2)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 6)) (2.9.0+cu126)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 7)) (4.57.2)\n",
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 8)) (5.1.2)\n",
      "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 9)) (3.8.11)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 10)) (3.9.1)\n",
      "Requirement already satisfied: rouge-score in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 11)) (0.1.2)\n",
      "Requirement already satisfied: bert-score in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 12)) (0.3.13)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 13)) (4.0.0)\n",
      "Requirement already satisfied: jiwer in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 14)) (4.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements.txt (line 2)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements.txt (line 2)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements.txt (line 2)) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 4)) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 4)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 4)) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 4)) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 4)) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 4)) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 4)) (3.2.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (3.6)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (3.5.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers->-r requirements.txt (line 7)) (0.36.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers->-r requirements.txt (line 7)) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers->-r requirements.txt (line 7)) (2025.11.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers->-r requirements.txt (line 7)) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers->-r requirements.txt (line 7)) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers->-r requirements.txt (line 7)) (0.7.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers->-r requirements.txt (line 8)) (1.6.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers->-r requirements.txt (line 8)) (1.16.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy->-r requirements.txt (line 9)) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy->-r requirements.txt (line 9)) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy->-r requirements.txt (line 9)) (1.0.15)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy->-r requirements.txt (line 9)) (2.0.13)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy->-r requirements.txt (line 9)) (3.0.12)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy->-r requirements.txt (line 9)) (8.3.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy->-r requirements.txt (line 9)) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy->-r requirements.txt (line 9)) (2.5.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy->-r requirements.txt (line 9)) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy->-r requirements.txt (line 9)) (0.4.3)\n",
      "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy->-r requirements.txt (line 9)) (0.20.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy->-r requirements.txt (line 9)) (2.12.3)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->-r requirements.txt (line 10)) (8.3.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk->-r requirements.txt (line 10)) (1.5.2)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge-score->-r requirements.txt (line 11)) (1.4.0)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge-score->-r requirements.txt (line 11)) (1.17.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets->-r requirements.txt (line 13)) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets->-r requirements.txt (line 13)) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets->-r requirements.txt (line 13)) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets->-r requirements.txt (line 13)) (0.70.16)\n",
      "Requirement already satisfied: rapidfuzz>=3.9.7 in /usr/local/lib/python3.12/dist-packages (from jiwer->-r requirements.txt (line 14)) (3.14.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 13)) (3.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers->-r requirements.txt (line 7)) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->-r requirements.txt (line 9)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->-r requirements.txt (line 9)) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->-r requirements.txt (line 9)) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers->-r requirements.txt (line 7)) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers->-r requirements.txt (line 7)) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers->-r requirements.txt (line 7)) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers->-r requirements.txt (line 7)) (2025.11.12)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->-r requirements.txt (line 6)) (1.3.0)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy->-r requirements.txt (line 9)) (1.3.3)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy->-r requirements.txt (line 9)) (0.1.5)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy->-r requirements.txt (line 9)) (0.23.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy->-r requirements.txt (line 9)) (7.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->-r requirements.txt (line 6)) (3.0.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers->-r requirements.txt (line 8)) (3.6.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 13)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 13)) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 13)) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 13)) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 13)) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 13)) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 13)) (1.22.0)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy->-r requirements.txt (line 9)) (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "724987c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m142.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK resources\n",
    "\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af031d23",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfcdb2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (5169, 4)\n",
      "Validation data shape: (861, 4)\n",
      "Test data shape: (862, 4)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "meetingbank = load_dataset(\"huuuyeah/meetingbank\")\n",
    "\n",
    "# Print the shape of the datasets\n",
    "print(f\"Train data shape: {meetingbank['train'].shape}\")\n",
    "print(f\"Validation data shape: {meetingbank['validation'].shape}\")\n",
    "print(f\"Test data shape: {meetingbank['test'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3595245a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Split the dataset into train, validation, and test sets and convert to pandas DataFrames\n",
    "train_df = pd.DataFrame(meetingbank['train'])\n",
    "val_df = pd.DataFrame(meetingbank['validation'])\n",
    "test_df = pd.DataFrame(meetingbank['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d970afa4",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "532925c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote: Enumerating objects: 8, done.\u001b[K\n",
      "remote: Counting objects: 100% (8/8), done.\u001b[K\n",
      "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
      "remote: Total 6 (delta 1), reused 6 (delta 1), pack-reused 0 (from 0)\u001b[K\n",
      "Unpacking objects: 100% (6/6), 1.67 KiB | 1.67 MiB/s, done.\n",
      "From https://github.com/shigenogoro/YouTube-Video-Summarization\n",
      " * branch            kyle       -> FETCH_HEAD\n",
      "   c961341..08c9e3c  kyle       -> origin/kyle\n",
      "Updating c961341..08c9e3c\n",
      "Fast-forward\n",
      " src/__init__.py                      | 0\n",
      " {notebooks/src => src}/proprocess.py | 0\n",
      " 2 files changed, 0 insertions(+), 0 deletions(-)\n",
      " create mode 100644 src/__init__.py\n",
      " rename {notebooks/src => src}/proprocess.py (100%)\n"
     ]
    }
   ],
   "source": [
    "# pull the latest changes\n",
    "!git pull origin {branch}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9eed52be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Determine if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "989a3be0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src.preprocess'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3612377200.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpreprocess_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Load the tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src.preprocess'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "from src.preprocess import preprocess_dataset\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "print(\"BART tokenizer loaded.\")\n",
    "\n",
    "# Load the pre-trained BART model for summarization and move to device\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained('facebook/bart-large-cnn').to(device)\n",
    "print(\"BART model loaded and moved to device.\")\n",
    "\n",
    "# Define the path for the preprocessed data\n",
    "preprocessed_data_path = 'data/meetingbank/preprocessed/'\n",
    "\n",
    "# Preprocess the entire dataset and save to .tsv files\n",
    "if not os.path.exists(preprocessed_data_path):\n",
    "    os.makedirs(preprocessed_data_path, exist_ok=True)\n",
    "    preprocess_dataset(train_df, os.path.join(preprocessed_data_path, os.path.join(preprocessed_data_path, 'train')))\n",
    "    preprocess_dataset(val_df, os.path.join(preprocessed_data_path, os.path.join(preprocessed_data_path, 'validation')))\n",
    "    preprocess_dataset(test_df, os.path.join(preprocessed_data_path, os.path.join(preprocessed_data_path, 'test')))\n",
    "    print(\"Data preprocessing completed and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0aff329",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# --- Import Preprocessing API ---\n",
    "# Make sure this path is correct relative to where your notebook is running\n",
    "from src.preprocess import preprocess_dataset \n",
    "# --- End Import Preprocessing API ---\n",
    "\n",
    "# Determine if GPU is available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "print(\"BART tokenizer loaded.\")\n",
    "\n",
    "# Load the pre-trained BART model for summarization and move to device\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\").to(device)\n",
    "print(\"BART model loaded and moved to device.\")\n",
    "\n",
    "# Define the path for the preprocessed test data\n",
    "preprocessed_test_dir = \"/content/drive/MyDrive/CS_685/Final_Project/data/preprocessed_test\"\n",
    "preprocessed_test_path = os.path.join(preprocessed_test_dir, \"preprocessed_data_0.csv\")\n",
    "\n",
    "# Check if the preprocessed data file exists. If not, trigger preprocessing.\n",
    "if not os.path.exists(preprocessed_test_path):\n",
    "    print(f\"Preprocessed test data not found at {preprocessed_test_path}. Starting preprocessing...\")\n",
    "    try:\n",
    "        # Load the test split from the MeetingBank dataset\n",
    "        meetingbank = load_dataset(\"huuuyeah/meetingbank\")\n",
    "        test_data = meetingbank['test']\n",
    "        print(\"MeetingBank test data loaded for preprocessing.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading MeetingBank dataset: {e}. Please ensure datasets library is installed and you have internet access.\")\n",
    "        raise # Re-raise the exception if dataset loading fails\n",
    "\n",
    "    # Call the external preprocessing function imported from preprocess.py\n",
    "    preprocess_dataset(test_data, output_dir=preprocessed_test_dir)\n",
    "    print(\"Preprocessing of test data complete.\")\n",
    "else:\n",
    "    print(f\"Preprocessed test data already exists at {preprocessed_test_path}.\")\n",
    "\n",
    "# Load the preprocessed test data sample\n",
    "test_df_sample = pd.read_csv(preprocessed_test_path)\n",
    "print(f\"Loaded {len(test_df_sample)} samples from {preprocessed_test_path}\")\n",
    "\n",
    "# Tokenize the 'transcript' column and prepare inputs for the model\n",
    "max_input_length = 1024\n",
    "max_summary_length = 150\n",
    "min_summary_length = 30\n",
    "batch_size = 16 # Define batch size for batched inference\n",
    "\n",
    "# Initialize a list to store results\n",
    "bart_results = []\n",
    "\n",
    "print(\"Generating summaries for the test data sample in batches...\")\n",
    "\n",
    "# Process in batches\n",
    "for i in tqdm(range(0, len(test_df_sample), batch_size), desc=\"Generating BART summaries\"):\n",
    "    batch = test_df_sample.iloc[i : i + batch_size]\n",
    "\n",
    "    batch_transcripts = batch['transcript'].tolist()\n",
    "    batch_original_summaries = batch['summary'].tolist()\n",
    "    batch_ids = batch['id'].tolist()\n",
    "\n",
    "    # Tokenize the batch of transcripts\n",
    "    inputs = tokenizer(\n",
    "        batch_transcripts,\n",
    "        max_length=max_input_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\", # Pad to max_input_length for batching\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device) # Move inputs to the same device as the model\n",
    "\n",
    "    # Generate summaries for the batch\n",
    "    summary_ids = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        num_beams=4,\n",
    "        max_length=max_summary_length,\n",
    "        min_length=min_summary_length,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    # Decode the generated summaries\n",
    "    generated_summaries = tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "    # Store results for each item in the batch\n",
    "    for j in range(len(batch_ids)):\n",
    "        bart_results.append({\n",
    "            'id': batch_ids[j],\n",
    "            'original_summary': batch_original_summaries[j],\n",
    "            'generated_summary': generated_summaries[j]\n",
    "        })\n",
    "\n",
    "print(\"Summary generation complete.\")\n",
    "\n",
    "# Convert results to a DataFrame for easier handling and evaluation later\n",
    "bart_baseline_df = pd.DataFrame(bart_results)\n",
    "print(f\"BART baseline results stored in DataFrame with {len(bart_baseline_df)} entries.\")\n",
    "print(\"First 5 BART generated summaries:\")\n",
    "print(bart_baseline_df[['id', 'original_summary', 'generated_summary']].head().to_string())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
